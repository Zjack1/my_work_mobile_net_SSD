首先，这两个文件有一个最大的不同点，train_val.prototxt 文件是网络配置文件，该文件是在训练的时候用的。deploy.prototxt 文件是在测试时使用的文件。下面以 Caffe 官方给出的 mnist 训练相关的文件作出详细注释说明：

name: "LeNet"
layer {
  name: "mnist"       #输入层的名称mnist
  type: "Data"        #输入层的类型data层
  top: "data"         #层的输出blob有两个：data和label
  top: "label"
  include {
    phase: TRAIN      #训练阶段，该层参数只在训练阶段有效
  }
  transform_param {
    scale: 0.00390625 #输入像素归一化到【0,1】 1/256=0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"  #LMDB的路径
    batch_size: 64                             #一次读取64张图
    backend: LMDB                              #数据格式为LMDB
  }
}
layer { #一个新数据层，名字也叫作mnist,输出blob也是data和label，但是这里定义的参数只在分类阶段有效
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST   #测试阶段
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size:100                 #batchsize大小，乘以test_iter = 测试集大小
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"     #本层使用上一层的data,生成下一层conv1的blob
  top: "conv1"
  param {
    lr_mult: 1       #权重参数w的学习率倍数，1表示保持与全局参数一致
  }
  param {
    lr_mult: 2       #偏置参数b的学习率倍数，是全局参数的2倍
  }
  convolution_param {
    num_output: 20      #输出单元数20
    kernel_size: 5      #卷积核大小为5*5
    stride: 1           #步长为1
    weight_filler {     #权值使用xavier填充器
      type: "xavier"    
    }
    bias_filler {       #bias使用常数填充器，默认为0
      type: "constant"  
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"     #本层的上一层是conv1，生成下一层Pool1的blob
  top: "pool1"
  pooling_param {     #下采样参数
    pool: MAX         #使用最大值下采样方法
    kernel_size: 2    #pooling核是2*2
    stride: 2         #pooling步长是2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {                    #新的全连接层，输入blob为pool2,输出blob为ip1
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {    #全连接层的参数
    num_output: 5          #输出500个节点
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {                   #新的非线性层，用RELU方法
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {                   #第二个全连接层
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10        #输出10个单元
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {    #分类准确率层（计算网络输出相对目标值的准确率），只在testing阶段有效，输入blob为iP2和label，输出blob为accuracy
  name: "accuracy"        #该层用于计算分类准确率
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {   #损失层，损失函数采用softmaxloss,输入blob为iP2和label，输出blob为loss
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
对比 deploy.prototxt 文件（此处不再列出）可以看出，deploy.prototxt 文件是在 train_val.prototxt 文件的基础上删除了一些东西，所形成的。train_val.prototxt 文件里面训练的部分都会在 deploy.prototxt 文件中删除。

train_val.prototxt	deploy.prototxt
开头要加入一下训练设置文件和准备文件。例如，transform_param 中的 mirror: true（开启镜像）；crop_size:（图像尺寸）；mean_file: “”（求解均值的文件），还有 data_param 中的source:”“（处理过得数据训练集文件）；batch_size:（训练图片每批次输入图片的数量）；backend: LMDB（数据格式设置）。然后，训练的时候还有一个测试的设置，测试和训练模式的设置通过一个 include{phase: TEST/TRAIN} 来设置。接下来就是要设置 TEST 模块内容。然后其他设置跟上面一样，里面有个 batch_size 可以调小一点，因为测试的话不需要特别多的图片数量。	只有一个数据层的设置。只需设置 name，type，top，input_param 这些即可。
第一个卷积层的设置, 多了param（反向传播学习率的设置），这里需要设置两个 param，一个是 weight 的学习率，一个是 bias 的学习率，其中一般 bias 的学习率是 weight 学习率的两倍	无
设置convolution_param, 需要有对weight_filler的初始化和对bias_filler的初始化。	无初始化操作
全连接层需要对 weight_filler 和 bias_filler 初始化。需要设置学习率	无初始化和学习率设置操作
接下来就是 Accuracy，这个层是用来计算网络输出相对目标值的准确率，它实际上并不是一个损失层，所以没有反传操作。	无
损失层 loss	将损失层 loss 改为 prob
--------------------- 
作者：HF飞哥 
来源：CSDN 
原文：https://blog.csdn.net/huangfei711/article/details/80723700 
版权声明：本文为博主原创文章，转载请附上博文链接！
